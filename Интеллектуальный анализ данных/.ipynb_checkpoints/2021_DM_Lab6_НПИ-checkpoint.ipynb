{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №6\n",
    "# Бинарная классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бинарная (двоичная) классификация (binary classification) — это задача классификации элементов заданного набора данных в два класса. \n",
    "\n",
    "Для бинарной классификации могут применяться методы многоклассовой классификации, а также более специализированные методы, такие, как метод опорных векторов, логистическая регрессия, линейный дискриминантный анализ и др."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия — это статистическая модель для бинарной классификации, использующая логистическую функцию (кривую). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая функция (сигмоида)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1. / (1. + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 500)\n",
    "\n",
    "plt.plot(x, sigmoid(x));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бинарная классификация при помощи логистической регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать набор данных Ирисы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставим в наборе данных первые два признака из четырех и первые два класса из трех:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[y<2,:2]\n",
    "y = y[y<2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем классы на плоскости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0], X[y==0,1], color=\"red\")\n",
    "plt.scatter(X[y==1,0], X[y==1,1], color=\"blue\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем набор данных на обучающую и тестовую выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим и обучим классификатор логистической регрессии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификатор не допускает ошибок на тестовой выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Граница решения "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задаче бинарной классификации граница решения (поверхность решения) - это гиперповерхность, которая разделяет пространство признаков на два набора, по одному для каждого класса. Классификатор классифицирует все точки по одной стороне границы принятия решения как принадлежащие одному классу, а все точки на другой стороне как принадлежащие другому классу. На самой границе решения прогнозируемая классификатором метка класса неоднозначна. \n",
    "\n",
    "Если граница решения является гиперплоскостью, то задача классификации линейна и классы линейно разделимы. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия - это линейный классификатор и граница решения характеризуется следующими коэффициентами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, прямая, разделяющая точки набора данных задается следующим образом: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x2(x1):\n",
    "    return (-log_reg.coef_[0][0] * x1 - log_reg.intercept_[0]) / log_reg.coef_[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем эту прямую и точки исходного набора данных и тестовой выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_plot = np.linspace(4, 7, 1000)\n",
    "x2_plot = x2(x1_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0], X[y==0,1], color=\"red\")\n",
    "plt.scatter(X[y==1,0], X[y==1,1], color=\"blue\")\n",
    "plt.plot(x1_plot, x2_plot);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[y_test==0,0], X_test[y_test==0,1], color=\"red\")\n",
    "plt.scatter(X_test[y_test==1,0], X_test[y_test==1,1], color=\"blue\")\n",
    "plt.plot(x1_plot, x2_plot);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать следующую функцию для визуализации границы решения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, axis):\n",
    "    \n",
    "    x0, x1 = np.meshgrid(\n",
    "        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),\n",
    "        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),\n",
    "    )\n",
    "    X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "    y_predict = model.predict(X_new)\n",
    "    zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n",
    "    \n",
    "    plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Граница решения для классификатора логистической регрессии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(log_reg, axis=[4, 7.5, 1.5, 4.5])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Граница решения для нелинейного классификатора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим классификатор на основе метода К ближайших соседей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Граница решения теперь является нелинейной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(knn_clf, axis=[4, 7.5, 1.5, 4.5])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим теперь классификатор на полном наборе данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf_all = KNeighborsClassifier()\n",
    "knn_clf_all.fit(iris.data[:,:2], iris.target);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Граница решения стала более извилистой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(knn_clf_all, axis=[4, 8, 1.5, 4.5])\n",
    "plt.scatter(iris.data[iris.target==0,0], iris.data[iris.target==0,1])\n",
    "plt.scatter(iris.data[iris.target==1,0], iris.data[iris.target==1,1])\n",
    "plt.scatter(iris.data[iris.target==2,0], iris.data[iris.target==2,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если метка класса определяется по 50 ближайшим соседям, то получим такую визуализацию границы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf_all = KNeighborsClassifier(n_neighbors=50)\n",
    "knn_clf_all.fit(iris.data[:,:2], iris.target)\n",
    "\n",
    "plot_decision_boundary(knn_clf_all, axis=[4, 8, 1.5, 4.5])\n",
    "plt.scatter(iris.data[iris.target==0,0], iris.data[iris.target==0,1])\n",
    "plt.scatter(iris.data[iris.target==1,0], iris.data[iris.target==1,1])\n",
    "plt.scatter(iris.data[iris.target==2,0], iris.data[iris.target==2,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод рекурсивного исключения признаков (RFE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод рекурсивного исключения признаков (recursive feature elimination, RFE) реализует следующий алгоритм: модель обучается на исходном наборе признаков и оценивает их значимость, затем исключается один или несколько наименее значимых признаков, модель обучается на оставшихся признаках, и так далее, пока не останется заданное количество лучших признаков. \n",
    "\n",
    "Метод RFE может применяться в сочетании с логистической регрессией для отбора лучших признаков: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction with RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# feature extraction\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "rfe = RFE(model)\n",
    "fit = rfe.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Число признаков: %d\" % fit.n_features_)\n",
    "print(\"Выбранные признаки: %s\" % fit.support_)\n",
    "print(\"Ранг признаков: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбранным (оцененым как лучшие) признакам присвоен ранг 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейный дискриминантный анализ (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод анализа основных компонентов PCA определяет комбинации признаков (основные компоненты), для которых набор данных имеет наибольшую дисперсию.\n",
    "\n",
    "Метод линейного дискриминантного анализа (LDA) определяет комбинации признаков, для которых классы разделяются наилучшим образом. LDA, в отличие от PCA, является методом машинного обучения с учителем, использующим заданные  метки классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit(X).transform(X)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_lda = lda.fit(X, y).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализации для методов PCA и LDA отличаются:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "colors = ['b', 'r', 'g']\n",
    "\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, alpha=.8, lw=2,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('Метод PCA для набора Ирисы');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_lda[y == i, 0], X_lda[y == i, 1], alpha=.8, color=color,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('Метод LDA для набора данных Ирисы');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В методе PCA определяются главные компоненты (комбинации признаков):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В методе LDA определяется направление (вектор), наилучшим образом разделяющий точки классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим метод LDA для классификации точек набора Ирисы (с двумя признаками и двумя классами):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Граница решения является линейной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(lda, axis=[4, 7.5, 1.5, 4.5])\n",
    "plt.scatter(X_test[y_test==0,0], X_test[y_test==0,1])\n",
    "plt.scatter(X_test[y_test==1,0], X_test[y_test==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод опорных векторов (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод опорных векторов (метод построения оптимальной разделяющей гиперплоскости) был разработан В. Вапником и А. Червоненкисом. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X = X[y<2,:2]\n",
    "y = y[y<2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0], X[y==0,1], color='red')\n",
    "plt.scatter(X[y==1,0], X[y==1,1], color='blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартизуем набор данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "standardScaler.fit(X)\n",
    "X_standard = standardScaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим и обучим классификатор на основе SVM с параметром регуляризации C=1e9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC(C=1e9)\n",
    "svc.fit(X_standard, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(svc, axis=[-3, 3, -3, 3])\n",
    "plt.scatter(X_standard[y==0,0], X_standard[y==0,1])\n",
    "plt.scatter(X_standard[y==1,0], X_standard[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим и обучим на тех же данных классификатор на основе SVM с другим параметром регуляризации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc2 = LinearSVC(C=10.)\n",
    "svc2.fit(X_standard, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(svc2, axis=[-3, 3, -3, 3])\n",
    "plt.scatter(X_standard[y==0,0], X_standard[y==0,1])\n",
    "plt.scatter(X_standard[y==1,0], X_standard[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Граница решения определяется такими параметрами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc2.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для визуализации зазора метода SVM будем использовать следующую функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_boundary(model, axis):\n",
    "    \n",
    "    x0, x1 = np.meshgrid(\n",
    "        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),\n",
    "        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),\n",
    "    )\n",
    "    X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "    y_predict = model.predict(X_new)\n",
    "    zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n",
    "    \n",
    "    plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "    \n",
    "    w = model.coef_[0]\n",
    "    b = model.intercept_[0]\n",
    "    \n",
    "    # w0*x0 + w1*x1 + b = 0\n",
    "    # => x1 = -w0/w1 * x0 - b/w1\n",
    "    plot_x = np.linspace(axis[0], axis[1], 200)\n",
    "    up_y = -w[0]/w[1] * plot_x - b/w[1] + 1/w[1]\n",
    "    down_y = -w[0]/w[1] * plot_x - b/w[1] - 1/w[1]\n",
    "    \n",
    "    up_index = (up_y >= axis[2]) & (up_y <= axis[3])\n",
    "    down_index = (down_y >= axis[2]) & (down_y <= axis[3])\n",
    "    plt.plot(plot_x[up_index], up_y[up_index], color='black')\n",
    "    plt.plot(plot_x[down_index], down_y[down_index], color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализации для первого и второго классификаторов имеют следующий вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc_decision_boundary(svc, axis=[-3, 3, -3, 3])\n",
    "plt.scatter(X_standard[y==0,0], X_standard[y==0,1])\n",
    "plt.scatter(X_standard[y==1,0], X_standard[y==1,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc_decision_boundary(svc2, axis=[-3, 3, -3, 3])\n",
    "plt.scatter(X_standard[y==0,0], X_standard[y==0,1])\n",
    "plt.scatter(X_standard[y==1,0], X_standard[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация нелинейного набора данных при помощи логистической регрессии\n",
    "\n",
    "Рассмотрим следующий синтетический набор данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(666)\n",
    "X = np.random.normal(0, 1, size=(200, 2))\n",
    "y = np.array((X[:,0]**2+X[:,1]**2)<1.5, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификация набора при помощи логистической регрессии дает низкое качество классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(log_reg, axis=[-4, 4, -4, 4])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полиномиальные зависимости\n",
    "\n",
    "Класс `PolynomialFeatures` позволяет строить наборы данных, содержащий полиномиальные зависимости от исходных данных (столбцы $x_1, x_2$ в наборе $D$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.arange(1, 11).reshape(-1, 2)\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "polyD = PolynomialFeatures(degree=2) # степень полинома\n",
    "polyD.fit(D)\n",
    "D_2 = polyD.transform(D)\n",
    "D_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем новый набор, в котором первый столбец соответствует свободному члену (не зависит от $x_1$, $x_2$), второй и третий столбцы равны $x_1$ и $x_2$, четвертый столбец равен $x_1^2$, пятый столбец равен $x_1 x_2$, шестой столбец равен $x_2^2$. \n",
    "\n",
    "Таким образом, от признаков $(x_1, x_2)$ переходим к расширенному набору признаков $(1, x_1, x_2, x_1^2, x_1 x_2, x_2^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение конвейера (Pipeline)\n",
    "\n",
    "Цель конвейера (pipeline) состоит в том, чтобы собрать несколько преобразований и заключительный классификатор, которые могут быть исполнены вместе с установкой различных параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def PolynomialLogisticRegression(degree):\n",
    "    return Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ('log_reg', LogisticRegression())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим конвейер с параметром `degree=2` для классификации нелинейного набора, приведенного выше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_log_reg = PolynomialLogisticRegression(degree=2)\n",
    "poly_log_reg.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_log_reg.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(poly_log_reg, axis=[-4, 4, -4, 4])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно увеличить параметр `degree` до 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_log_reg2 = PolynomialLogisticRegression(degree=20)\n",
    "poly_log_reg2.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_log_reg2.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(poly_log_reg2, axis=[-4, 4, -4, 4])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параметры логистической регрессии\n",
    "\n",
    "Реализация логистической регрессии в __sklearn__ допускает некоторое количество параметров, в частности:\n",
    "\n",
    "* `penalty` - штраф, принимающий значения ‘l1’, ‘l2’, ‘elasticnet’ или ‘none’\n",
    "* `C` - константа регуляризации (обратно пропорциональна силе регуляризации)\n",
    "* `solver` - алгоритм, используемый для оптимизации, принимающий значения ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’\n",
    "\n",
    "Рассмотрим другой синтетический набор данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(0, 1, size=(200, 2))\n",
    "y = np.array((X[:,0]**2+X[:,1])<1.5, dtype='int')\n",
    "for _ in range(20):\n",
    "    y[np.random.randint(200)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.score(X_train, y_train), log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(log_reg, axis=[-4, 4, -4, 4])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialLogisticRegression(degree=2, C=1.):\n",
    "    return Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ('log_reg', LogisticRegression(C=C))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_log_reg3 = PolynomialLogisticRegression(degree=2, C=0.1)\n",
    "poly_log_reg3.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_log_reg3.score(X_train, y_train), poly_log_reg3.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация нелинейного набора данных при помощи SVM\n",
    "\n",
    "Создадим нелинейный набор данных в форму пулумесяцев:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.make_moons()\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим шум:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(noise=0.15, random_state=666)\n",
    "\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим конвейер на основе SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialSVC(degree, C=1.0):\n",
    "    return Pipeline([\n",
    "        (\"poly\", PolynomialFeatures(degree=degree)),\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "        (\"linearSVC\", LinearSVC(C=C))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_svc = PolynomialSVC(degree=3)\n",
    "poly_svc.fit(X, y)\n",
    "poly_svc.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(poly_svc, axis=[-1.5, 2.5, -1.0, 1.5])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Матрица ошибок, точность и полнота"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продемонстрируем меры бинарной классификации на примере набора данных с изображениеми цифр:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target.copy()\n",
    "\n",
    "y[digits.target==9] = 1\n",
    "y[digits.target!=9] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим классификатор логистической регрессии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log_predict = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Показатели TN, FP, FN и TP могут быть реализованы так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TN(y_true, y_predict):\n",
    "    assert len(y_true) == len(y_predict)\n",
    "    return np.sum((y_true == 0) & (y_predict == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FP(y_true, y_predict):\n",
    "    assert len(y_true) == len(y_predict)\n",
    "    return np.sum((y_true == 0) & (y_predict == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FN(y_true, y_predict):\n",
    "    assert len(y_true) == len(y_predict)\n",
    "    return np.sum((y_true == 1) & (y_predict == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TP(y_true, y_predict):\n",
    "    assert len(y_true) == len(y_predict)\n",
    "    return np.sum((y_true == 1) & (y_predict == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрица ошибок для бинарной классификации определяется так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_predict):\n",
    "    return np.array([\n",
    "        [TN(y_true, y_predict), FP(y_true, y_predict)],\n",
    "        [FN(y_true, y_predict), TP(y_true, y_predict)]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Показатель точности для положительного класса может быть вычислен так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score(y_true, y_predict):\n",
    "    tp = TP(y_true, y_predict)\n",
    "    fp = FP(y_true, y_predict)\n",
    "    try:\n",
    "        return tp / (tp + fp)\n",
    "    except:\n",
    "        return 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Показатель TPR (доля корректно спрогнозированных положительных точек) вычисляется так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_score(y_true, y_predict):\n",
    "    tp = TP(y_true, y_predict)\n",
    "    fn = FN(y_true, y_predict)\n",
    "    try:\n",
    "        return tp / (tp + fn)\n",
    "    except:\n",
    "        return 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании функций из scikit-learn получим аналогичные результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision_score(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall_score(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кривая полнота-точность (precision-recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кривая полнота-точность строится в координатах полнота (recall) и точность (precision). Площадь под кривой часто используют в качестве метрики качества алгоритма. Площадь под кривой полнота-точность  рекомендуют использовать в задачах с дисбалансом классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target.copy()\n",
    "\n",
    "y[digits.target==9] = 1\n",
    "y[digits.target!=9] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "decision_scores = log_reg.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "thresholds = np.arange(np.min(decision_scores), np.max(decision_scores), 0.1)\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in thresholds:\n",
    "    y_predict = np.array(decision_scores >= threshold, dtype='int')\n",
    "    precisions.append(precision_score(y_test, y_predict))\n",
    "    recalls.append(recall_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thresholds, precisions, c='r')\n",
    "plt.plot(thresholds, recalls, c='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(precisions, recalls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, decision_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thresholds, precisions[:-1])\n",
    "plt.plot(thresholds, recalls[:-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(precisions, recalls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ ROC кривых"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Площадь под ROC-кривой – одна из самых популярных мер качества в задачах бинарной классификации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target.copy()\n",
    "\n",
    "y[digits.target==9] = 1\n",
    "y[digits.target!=9] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для ROC-анализа нужны значения т.н. скоринговой функции для каждой точки в тестовом наборе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "decision_scores = log_reg.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вдоль осей откладываются показатели FPR (ось абсцисс) и TPR (ось ординат):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprs = []\n",
    "tprs = []\n",
    "n = len(X_test)\n",
    "thresholds = np.arange(np.min(decision_scores), np.max(decision_scores), 0.1)\n",
    "for threshold in thresholds:\n",
    "    y_predict = np.array(decision_scores >= threshold, dtype='int')\n",
    "    fprs.append(FP(y_test, y_predict)/n)\n",
    "    tprs.append(TP(y_test, y_predict)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fprs, tprs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование библиотечной функции дает тот же результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fprs, tprs, thresholds = roc_curve(y_test, decision_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fprs, tprs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для вычисления площади под кривой можно использовать метод трапеций или библиотечную функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test, decision_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание на лабораторную работу №6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для закрепленного за Вами варианта лабораторной работы:\n",
    "\n",
    "1.\tСчитайте заданный набор данных из репозитария UCI. \n",
    "\n",
    "2.\tЕсли среди меток класса имеются пропущенные значения, то удалите записи с пропущенными метками класса. Если какие-либо числовые признаки в наборе были распознаны неверно, то преобразуйте их в числовые. Оставьте в наборе данных только числовые признаки. Если в признаках имеются пропущенные значения, то замените их на медианные значения признака. \n",
    "\n",
    "3.\tВычислите и визуализируйте матрицу корреляций признаков. Удалите из набора признаки, имеющие высокую корреляцию (близкую к +1 или -1) с другими признаками. \n",
    "\n",
    "4.\tЕсли столбец с метками классов содержит более двух классов, то объедините некоторые классы, чтобы получить набор для бинарной классификации. Объединяйте классы таким образом, чтобы положительный и отрицательный классы были сопоставимы по количеству точек. \n",
    "\n",
    "5.\tИспользуя метод рекурсивного исключения признаков (RFE) и логистическую регрессию, определите и оставьте в наборе наиболее значимые признаки (не менее двух). Если в наборе данных осталось более двух признаков, то определите два признака с наибольшей дисперсией для визуализации.\n",
    "\n",
    "6.\tМасштабируйте признаки набора данных на интервал от 0 до 1.\n",
    "\n",
    "7.\tИспользуя разделение набора данных на обучающую и тестовую выборки в соотношении 70% на 30%, создайте и обучите классификаторы на основе: \n",
    "\n",
    "* наивного байесовского классификатора \n",
    "* логистической регрессии \n",
    "* логистической регрессии с полиномиальными зависимостями (`degree` равно 2 и 3)\n",
    "* линейного дискриминантного анализа  \n",
    "* метода опорных векторов\n",
    "* метода опорных векторов с полиномиальными зависимостями (`degree` равно 2 и 3)\n",
    "\n",
    "8. \tВизуализируйте для каждого из классификаторов границу решения, подписывая оси и рисунок и создавая легенду для меток классов набора данных. \n",
    "\n",
    "9. \tВизуализируйте на одном рисунке ROC кривые для каждого из классификаторов, подписывая оси и рисунок и создавая легенду для методов бинарной классификации. \n",
    "\n",
    "10.\tОпределите лучший метод бинарной классификации набора данных по показателю ROC_AUC (площади под ROC кривой). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
