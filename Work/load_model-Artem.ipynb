{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "sys.path.append('/home/u2/debug_scoring')\n",
    "# sys.path.append('/home/u2/scoring')\n",
    "from scoring import Agreements, Statuses, Features\n",
    "\n",
    "from FeaturesSelection.lib.features_selection import FeaturesSelection\n",
    "from Sampling.lib.sampling import Sampling as Sampling\n",
    "from Modeling.lib.modeling import Modeling\n",
    "from Metrics.lib.metrics import Metrics\n",
    "\n",
    "modeling = Modeling(n_jobs=-1, accuracy_threshold=0.6, log=False)\n",
    "features_selection = FeaturesSelection()\n",
    "sampling = Sampling()\n",
    "metrics = Metrics()\n",
    "\n",
    "# pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_card(name_card,\n",
    "              id_card = '_v0',\n",
    "              load_path: str = 'models/',):\n",
    "    \"\"\"\n",
    "    Загрузка результатов моделирования\n",
    "    \"\"\"\n",
    "    load_path +=name_card+'/'\n",
    "    with open(f'{load_path}{name_card}{id_card}.sav', 'rb') as f:\n",
    "        data_load = pickle.load(f)\n",
    "    return data_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_sav(info_model):\n",
    "    \"\"\"\n",
    "    Распаковка sav файла\n",
    "    \"\"\"\n",
    "    model = info_model['model']\n",
    "    # Подгрузка переменных из sav файла\n",
    "    predict_list = info_model['predict_list']\n",
    "    research_end = info_model['research_end']\n",
    "    dict_params = info_model['all_params']\n",
    "    df_features_importance = info_model['features_importance']\n",
    "\n",
    "\n",
    "    test_period = dict_params['test_period']\n",
    "    status = dict_params['status']\n",
    "    cnt_loans = dict_params['cnt_loans']\n",
    "    loan_id_status = dict_params['loan_id_status']\n",
    "    num = dict_params['num']\n",
    "    num_online = dict_params['num_online']\n",
    "    prolong = dict_params['prolong']\n",
    "    model_type = dict_params['model_type']\n",
    "    target_name = dict_params['target_name']\n",
    "    if 'decision_process_type' in dict_params:\n",
    "        decision_process_type = dict_params['decision_process_type']\n",
    "\n",
    "    # Получение данных для eval_set\n",
    "    # Импорт данных\n",
    "    # API\n",
    "#     if research_start is None and research_end is None:\n",
    "        research_start = research_end\n",
    "        # research_start = datetime.strptime('2021-11-03  00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "#         test_period = 20\n",
    "        research_end = research_start + timedelta(test_period)\n",
    "    #     research_end = datetime.strptime('2021-11-11  00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    if 'decision_process_type' in dict_params:\n",
    "        agreements = Agreements(delay=status,\n",
    "                                research_start=research_start,\n",
    "                                research_end=research_end,\n",
    "                                num_online=num_online,\n",
    "                                num=num,\n",
    "                                loan_id=loan_id_status,\n",
    "                                cnt_loans=cnt_loans,\n",
    "                                prolong=prolong,\n",
    "                                decision_process_type = decision_process_type).get()\n",
    "        print(decision_process_type)\n",
    "    else:\n",
    "        agreements = Agreements(delay=status,\n",
    "                                research_start=research_start,\n",
    "                                research_end=research_end,\n",
    "                                num_online=num_online,\n",
    "                                num=num,\n",
    "                                loan_id=loan_id_status,\n",
    "                                cnt_loans=cnt_loans,\n",
    "                                prolong=prolong).get()\n",
    "            \n",
    "    agreements = agreements.dropna(axis = 0,subset = ['nbki_cs__cnt_loans'])\n",
    "    # agreements = agreements.fillna(0)\n",
    "    get_status = Statuses(dataset=agreements,\n",
    "                          target=status).get(prolong=prolong)\n",
    "\n",
    "    f = Features(dataset=agreements,\n",
    "                 data_type=model_type)\n",
    "    get_features = f.get()\n",
    "\n",
    "    df_type = f.get_features_list()\n",
    "\n",
    "    # Соединение предикторов и таргетов в единый датарфейм\n",
    "    df = pd.merge(get_status, get_features, left_index=True, right_index=True)\n",
    "    df_app_id = df['app_id'].astype('str')\n",
    "    df = df.drop(['app_id'], axis=1).join(agreements['creation_date'])\n",
    "\n",
    "    print(research_start)\n",
    "    print(research_end)\n",
    "    print(df.shape)\n",
    "\n",
    "    # dummies для eval_set\n",
    "\n",
    "    # Получение отобранных фич\n",
    "    df = df[predict_list + [target_name]]\n",
    "    # Получение данных биннинга из sav файла\n",
    "    df_woe = info_model['df_woe']\n",
    "    # фичи, по которым нуждно сделать биннинг\n",
    "    mas_predict_to_woe = info_model['mas_predict_to_woe']\n",
    "    \n",
    "    # Замена значений на промежутки\n",
    "    df.loc[:, mas_predict_to_woe] = features_selection.interval_definition(df.loc[:, mas_predict_to_woe],\n",
    "                                                                                 df_woe[df_woe['column_name'].isin(\n",
    "                                                                                     mas_predict_to_woe)]).copy()\n",
    "    # Применение dummies\n",
    "    df = pd.get_dummies(data=df, columns=mas_predict_to_woe)\n",
    "\n",
    "    # Если получилось так, что не хватает нескольких столбцов биннинга, то добавляем эти столбцы с значением 0\n",
    "    mas_predict_incompatible = [i for i in info_model['predict_list_binning'] if i not in df.columns]\n",
    "    if len(mas_predict_incompatible) != 0:\n",
    "        df.loc[:,mas_predict_incompatible] = 0\n",
    "\n",
    "    # Разбиение на X и y\n",
    "    y = df[[target_name]]\n",
    "    X = df.drop([target_name],axis = 1)\n",
    "\n",
    "    # Сортировка как в X_train\n",
    "    X= X[info_model['predict_list_binning']]\n",
    "\n",
    "    # получение серии со значением features_importance по модулю для изображения на графике\n",
    "    ser_features_importance = df_features_importance.copy()\n",
    "    ser_features_importance = ser_features_importance.set_index(ser_features_importance['features'])\n",
    "    ser_features_importance = ser_features_importance['_coef']\n",
    "    return X,y,model,ser_features_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data(df,df_type,info_model):\n",
    "    \"\"\"\n",
    "    Распаковка sav файла\n",
    "    \"\"\"\n",
    "    target_name = info_model['all_params']['target_name']\n",
    "    # Получение отобранных фич\n",
    "    df = df[info_model['predict_list'] + [target_name]]\n",
    "    # Получение данных биннинга из sav файла\n",
    "    df_woe = info_model['df_woe']\n",
    "    # фичи, по которым нуждно сделать биннинг\n",
    "    mas_predict_to_woe = info_model['mas_predict_to_woe']\n",
    "    \n",
    "    # Замена значений на промежутки\n",
    "    df.loc[:, mas_predict_to_woe] = features_selection.interval_definition(df.loc[:, mas_predict_to_woe],\n",
    "                                                                                 df_woe[df_woe['column_name'].isin(\n",
    "                                                                                     mas_predict_to_woe)]).copy()\n",
    "    # Применение dummies\n",
    "    df = pd.get_dummies(data=df, columns=mas_predict_to_woe)\n",
    "\n",
    "    # Если получилось так, что не хватает нескольких столбцов биннинга, то добавляем эти столбцы с значением 0\n",
    "    mas_predict_incompatible = [i for i in info_model['predict_list_binning'] if i not in df.columns]\n",
    "    if len(mas_predict_incompatible) != 0:\n",
    "        df.loc[:,mas_predict_incompatible] = 0\n",
    "\n",
    "    # Разбиение на X и y\n",
    "    y = df[[target_name]]\n",
    "    X = df.drop([target_name],axis = 1)\n",
    "\n",
    "    # Сортировка как в X_train\n",
    "    X= X[info_model['predict_list_binning']]\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_select_type(dict_params,\n",
    "                        type_selection,\n",
    "                        decision_process_type,\n",
    "                        dev,\n",
    "                        df = None,\n",
    "                        research_start = None,\n",
    "                        research_end = None,\n",
    "                        use_build_data_for_valid = True,\n",
    "                        set_app_id_as_index = False):\n",
    "    \"\"\"\n",
    "    Отбор множеств по указанным параметрам\n",
    "    \n",
    "    df - датафрейм который идет в сохранение с столбцами app_id и type_selection\n",
    "    use_build_data_for_valid - использовать ли выборку из билда\n",
    "    \"\"\"\n",
    "    # Если старотвая дата не указана\n",
    "    if research_start is None:\n",
    "         # Выборка из билда если необходимо\n",
    "        if use_build_data_for_valid:\n",
    "            research_start = dict_params['all_params']['research_start']\n",
    "        else:\n",
    "            research_start = dict_params['research_end'] - timedelta(dict_params['all_params']['test_period'])\n",
    "    if research_end is None:\n",
    "        research_end       = dict_params['research_end']\n",
    "        \n",
    "    # Выборка из билда если необходимо\n",
    "    dev_table = True\n",
    "    if dev is None:\n",
    "        dev_table = False\n",
    "    \n",
    "    print(research_start)\n",
    "    print(research_end)\n",
    "    agreements = Agreements(delay                 = dict_params['all_params']['status'],\n",
    "                            research_start        = research_start,\n",
    "                            research_end          = research_end,\n",
    "                            num_online            = dict_params['all_params']['num_online'],\n",
    "                            num                   = dict_params['all_params']['num'],\n",
    "                            loan_id               = dict_params['all_params']['loan_id_status'],\n",
    "                            cnt_loans             = dict_params['all_params']['cnt_loans'],\n",
    "                            prolong               = dict_params['all_params']['prolong'],\n",
    "                            decision_process_type = decision_process_type,\n",
    "                            add_features          = dict_params['all_params']['model_type'],\n",
    "                            model_type            = dict_params['all_params']['model_type'],\n",
    "                            dev_table             = dev_table,\n",
    "                            dev_table_postfix     = dev,\n",
    "                            \n",
    "                            db_name='scoring3').get()\n",
    "    agreements = agreements.dropna(axis = 0,subset = ['nbki_cs__cnt_loans'])\n",
    "    \n",
    "    get_status = Statuses(dataset=agreements,\n",
    "                          target=dict_params['all_params']['status']).get(prolong=dict_params['all_params']['prolong'])\n",
    "\n",
    "\n",
    "    f = Features(dataset=agreements,\n",
    "                 data_type=dict_params['all_params']['model_type'])\n",
    "    get_features = f.get()\n",
    "\n",
    "    df_type1 = f.get_features_list()\n",
    "\n",
    "    # Соединение предикторов и таргетов в единый датарфейм\n",
    "    df1 = pd.merge(get_status, get_features, left_index=True, right_index=True)\n",
    "    df_app_id = df1['app_id'].astype('str')\n",
    "    df1 = df1.drop(['app_id'], axis=1).join(agreements['creation_date']) \n",
    "    \n",
    "    print(df1.shape)\n",
    "    if df is None:\n",
    "        X,y = processing_data(df1,df_type1,info_model)\n",
    "        return X,y\n",
    "    \n",
    "    # отбор записей с указанным типом и decision_process_type\n",
    "    select_app_id = df['app_id'].isin(get_status['app_id'])\n",
    "    \n",
    "    select_set = (df['type_selection'] == type_selection)\n",
    "    df = df[select_app_id & select_set]\n",
    "    \n",
    "    if set_app_id_as_index:\n",
    "        # удаление ненужных столбцов\n",
    "        df = df.drop(['predict_proba_0','status_predict','type_selection'],axis = 1)\n",
    "        df = df.set_index('app_id')\n",
    "    else:\n",
    "        # удаление ненужных столбцов\n",
    "        df = df.drop(['predict_proba_0','status_predict','type_selection','app_id'],axis = 1)\n",
    "\n",
    "    # Выборка наборов\n",
    "    X = df.drop(['status'],axis = 1)\n",
    "    y = df[['status']]\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(name_card, id_card):\n",
    "    # Путь\n",
    "    path = f'samples/{name_card}/{id_card}/'\n",
    "    # Имя датафрейма\n",
    "    name_data = os.listdir(path)[0]\n",
    "    path += name_data\n",
    "\n",
    "    # Выгрузка\n",
    "    data = pd.read_csv(path,index_col = 0,dtype = {'app_id':str})\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_cum_sum(X,y,model,size,inters,treshold):\n",
    "    mas_result = []\n",
    "    X_main             = X.copy()\n",
    "    X_main['status']   = y['status'].copy()\n",
    "\n",
    "    for i in range(inters):\n",
    "\n",
    "        # Генерация выборки\n",
    "        X = X_main.copy()\n",
    "        X = X.sample(n = size)\n",
    "#         print(X.shape)\n",
    "        y = X[['status']]\n",
    "        X = X.drop(['status'],axis = 1)\n",
    "\n",
    "        # получаем датафрейм просрочки\n",
    "        df_cumsum = metrics.get_cum_sum(X, y, model)\n",
    "        df_cumsum = df_cumsum[df_cumsum['predict_proba'] >= treshold]\n",
    "#         display(df_cumsum)\n",
    "\n",
    "        # Выор значений\n",
    "        sum_credits  = None\n",
    "        cum_sum      = None\n",
    "        n_credit_prt = None\n",
    "        if df_cumsum.shape[0] != 0:\n",
    "            sum_credits = df_cumsum['n_credits'].sum()\n",
    "            cum_sum      = df_cumsum.iloc[0]['cum_sum']\n",
    "            n_credit_prt = df_cumsum.iloc[0]['n_credit_prt']\n",
    "\n",
    "        # Запись в массив\n",
    "        dict_record = {'n_credits':sum_credits,\n",
    "                      'cum_sum':cum_sum,\n",
    "                      'n_credit_prt':n_credit_prt}\n",
    "        mas_result.append(dict_record)\n",
    "\n",
    "    df_result = pd.DataFrame(mas_result)\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(X,model,treshold = 0.5):\n",
    "    \"\"\"\n",
    "    Получение предсказаний\n",
    "    \"\"\"\n",
    "    # predict_proba для 0\n",
    "    predict_proba_0 = model.predict_proba(X).T[0]\n",
    "    \n",
    "    if treshold is None:\n",
    "        y_predict = model.predict(X)\n",
    "    else:\n",
    "        y_predict = list(map(lambda x : 0 if x >=treshold else 1,predict_proba_0))\n",
    "    \n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data_of_prediction(X,y,model,treshold = 0.5,target_predict = 1):\n",
    "    \"\"\"\n",
    "    Отбор данных, для которых был предсказан 1 или 0\n",
    "    \"\"\"\n",
    "    # предсказания\n",
    "    predict = model_predict(X = X,\n",
    "                            model = model,\n",
    "                            treshold = treshold)\n",
    "    # Проверка условия\n",
    "    mas_bool_pred = [val == target_predict for val in predict]\n",
    "    \n",
    "    X = X[mas_bool_pred]\n",
    "    y = y[mas_bool_pred]\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data(params = None,app_ids = None,dev = None):\n",
    "    \n",
    "#     if dev is None:\n",
    "#         dev_table = None\n",
    "#         dev_table_postfix = None\n",
    "#         # Проверка на dev\n",
    "#         if 'dev' in params:\n",
    "#             dev_table = True\n",
    "#             dev_table_postfix = params['dev']\n",
    "#             if dev_table_postfix is None:\n",
    "#                 dev_table = False\n",
    "#         else:\n",
    "#             dev_table = False\n",
    "#             dev_table_postfix = None \n",
    "#     else:\n",
    "#         dev_table = False\n",
    "#         dev_table_postfix = None\n",
    "\n",
    "#     if \n",
    "#     agreements = Agreements(delay                 = params['status'],\n",
    "#                             research_start        = params['research_start'],\n",
    "#                             research_end          = params['research_end'],\n",
    "#                             num_online            = params['num_online'],\n",
    "#                             num                   = params['num'],\n",
    "#                             loan_id               = params['loan_id_status'],\n",
    "#                             cnt_loans             = params['cnt_loans'],\n",
    "#                             prolong               = params['prolong'],\n",
    "#                             decision_process_type = params['decision_process_type'],\n",
    "#                             add_features          = params['model_type'],\n",
    "#                             model_type            = params['model_type'],\n",
    "#                             dev_table             = dev_table,\n",
    "#                             dev_table_postfix     = dev_table_postfix,\n",
    "                            \n",
    "#                             db_name='scoring3').get()\n",
    "#     agreements = agreements.dropna(axis = 0,subset = ['nbki_cs__cnt_loans'])\n",
    "    \n",
    "#     get_status = Statuses(dataset=agreements,\n",
    "#                           target=params['status']).get(prolong=params['prolong'])\n",
    "\n",
    "\n",
    "#     f = Features(dataset=agreements,\n",
    "#                  data_type=params['model_type'])\n",
    "#     get_features = f.get()\n",
    "\n",
    "#     df_type1 = f.get_features_list()\n",
    "\n",
    "#     # Соединение предикторов и таргетов в единый датарфейм\n",
    "#     df1 = pd.merge(get_status, get_features, left_index=True, right_index=True)\n",
    "    \n",
    "#     return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_app_ids(app_ids = None,status = 3,prolong = True,model_type = ['equifax_cs','nbki_cs'],dev = None):\n",
    "    \n",
    "    dev_table = False\n",
    "    if dev is not None:\n",
    "        dev_table = True\n",
    "\n",
    "    agreements = Agreements(app_ids = app_ids,\n",
    "                            db_name = 'scoring3',\n",
    "                            dev_table = dev_table,\n",
    "                            dev_table_postfix = dev).get()\n",
    "    agreements = agreements.dropna(axis = 0,subset = ['nbki_cs__cnt_loans'])\n",
    "    get_status = Statuses(dataset=agreements,target=status).get(prolong=prolong)\n",
    "    f = Features(dataset=agreements,data_type=model_type)\n",
    "    get_features = f.get()\n",
    "\n",
    "    df_type = f.get_features_list()\n",
    "\n",
    "    # Соединение предикторов и таргетов в единый датарфейм\n",
    "    df = pd.merge(get_status, get_features, left_index=True, right_index=True)\n",
    "    \n",
    "    return df,df_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка моделей и данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_card = 'RF'\n",
    "name_card_1 = 'LR'\n",
    "\n",
    "# Модель\n",
    "id_card_1   = '_v1647260486'\n",
    "\n",
    "# Подгрузка sav файла\n",
    "info_model = load_card(name_card = name_card_1,id_card = id_card_1)\n",
    "model = info_model['model']\n",
    "\n",
    "# Загрузка данных на которых обучалась модель\n",
    "df_full = load_data(name_card = name_card_1,id_card = id_card_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192.168.14.169 scoring3\n",
      "[INFO]: Ошибка получения контекста...закрытие соединения с БД\n",
      "192.168.14.169 scoring3\n",
      "[INFO]: Ошибка получения контекста...закрытие соединения с БД\n"
     ]
    }
   ],
   "source": [
    "# Получение данных по app_id\n",
    "mas_app_id = ['000031942']\n",
    "df,df_type = get_data_app_ids(mas_app_id,\n",
    "                               status = 3,\n",
    "                               prolong = True,\n",
    "                               model_type = ['equifax_cs','nbki_cs'],\n",
    "                               dev = 'dev2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обработка фичей для подачи в модель\n",
    "X,y = processing_data(df = df,\n",
    "                               df_type = df_type,\n",
    "                               info_model = info_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>equifax_cs__cnt_delay5_micr</th>\n",
       "      <th>equifax_cs__cnt_request_microcredits_quarter</th>\n",
       "      <th>equifax_cs__max_request_microcredits_day</th>\n",
       "      <th>equifax_cs__mean_request_microcredits_hour</th>\n",
       "      <th>equifax_cs__min_request_microcredits_week</th>\n",
       "      <th>equifax_cs__share_of_microloans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>0.998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   equifax_cs__cnt_delay5_micr  equifax_cs__cnt_request_microcredits_quarter  \\\n",
       "0                            0                                            10   \n",
       "\n",
       "   equifax_cs__max_request_microcredits_day  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   equifax_cs__mean_request_microcredits_hour  \\\n",
       "0                                         0.0   \n",
       "\n",
       "   equifax_cs__min_request_microcredits_week  equifax_cs__share_of_microloans  \n",
       "0                                    16000.0                            0.998  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>_coef</th>\n",
       "      <th>iv</th>\n",
       "      <th>_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>equifax_cs__share_of_microloans</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>equifax_cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>equifax_cs__cnt_delay5_micr</td>\n",
       "      <td>0.194135</td>\n",
       "      <td>0.194135</td>\n",
       "      <td>equifax_cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>equifax_cs__cnt_request_microcredits_quarter</td>\n",
       "      <td>-0.134268</td>\n",
       "      <td>0.134268</td>\n",
       "      <td>equifax_cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>equifax_cs__mean_request_microcredits_hour</td>\n",
       "      <td>-0.000292</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>equifax_cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>equifax_cs__max_request_microcredits_day</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>equifax_cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>equifax_cs__min_request_microcredits_week</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>equifax_cs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       features     _coef        iv  \\\n",
       "0               equifax_cs__share_of_microloans  1.000000  1.000000   \n",
       "1                   equifax_cs__cnt_delay5_micr  0.194135  0.194135   \n",
       "2  equifax_cs__cnt_request_microcredits_quarter -0.134268  0.134268   \n",
       "3    equifax_cs__mean_request_microcredits_hour -0.000292  0.000292   \n",
       "4      equifax_cs__max_request_microcredits_day -0.000138  0.000138   \n",
       "5     equifax_cs__min_request_microcredits_week -0.000095  0.000095   \n",
       "\n",
       "        _pred  \n",
       "0  equifax_cs  \n",
       "1  equifax_cs  \n",
       "2  equifax_cs  \n",
       "3  equifax_cs  \n",
       "4  equifax_cs  \n",
       "5  equifax_cs  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _coef = важность фичей\n",
    "# iv = важность фичей по модулю\n",
    "# _pred - источник фичи\n",
    "info_model['features_importance']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
